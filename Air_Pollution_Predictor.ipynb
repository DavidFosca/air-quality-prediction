{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Air_Pollution_Predictor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2HDb6nx89xa"
      },
      "source": [
        "**Importar LibrerÃ­as**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Klr-COfaQK00"
      },
      "source": [
        "import pandas as pd\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.models import Sequential    \n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "from sklearn.metrics import mean_squared_error   # para calcular el error cuadratico medio\n",
        "from math import sqrt\n",
        "\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldEK2_wIg9Ov"
      },
      "source": [
        "**Preprocesing Data - Functions:**\n",
        "1. Drop outliers.\n",
        "2. Standardizing values.\n",
        "3. Dealing with missing values. \n",
        "4. Feature Engineering - Differencing.\n",
        "5. Feature Engineering - Computing Time Features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHXPVYVKoWcv"
      },
      "source": [
        "#Remove Outliers:\n",
        "def remove_outliers(dataframe):\n",
        "  data = dataframe.copy()\n",
        "  data.drop(columns=['Latitud', 'Longitud', 'Fecha', 'Ruido (dB)', 'UV', 'Presion (hPa)'], inplace=True)\n",
        "  for (columnName, columnData) in data.iteritems():\n",
        "      feature = data[columnName]\n",
        "      #Search for outliers:\n",
        "      outliers = feature.between(feature.quantile(.1), feature.quantile(.9))\n",
        "      index_names = data[~outliers].index\n",
        "      #Replace outliers with word \"NaN\":\n",
        "      data[columnName].loc[index_names] = np.nan\n",
        "      #print(outliers.value_counts())\n",
        "      #print(index_names)\n",
        "  return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfsBzQrlxq86"
      },
      "source": [
        "#Computing Standard Vector.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "def standarized_vector(dataframe):\n",
        "  data = dataframe.copy()\n",
        "  data.drop(columns=['Latitud', 'Longitud', 'Fecha'], inplace=True)\n",
        "  vector_scaler = StandardScaler()\n",
        "  vector_scaler.fit_transform(data)\n",
        "  return vector_scaler"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3kSnLn2mOLx"
      },
      "source": [
        "#Standarizing Values.\r\n",
        "def standarized_values(dataframe, vector_scaler):\r\n",
        "  data = dataframe.copy()\r\n",
        "  data.drop(columns=['Latitud', 'Longitud', 'Fecha'], inplace=True)\r\n",
        "  df_standard_scale = pd.DataFrame(vector_scaler.transform(data))\r\n",
        "  df_standard_scale = df_standard_scale.rename(columns={0: \"CO (ug/m3)\", 1: \"H2S (ug/m3)\", 2: \"NO2 (ug/m3)\", 3: \"O3 (ug/m3)\", 4: \"PM10 (ug/m3)\", 5: \"PM2.5 (ug/m3)\", 6: \"SO2 (ug/m3)\", 7: \"Ruido (dB)\", 8: \"UV\", 9: \"Humedad (%)\", 10: \"Presion (hPa)\", 11: \"Temperatura (C)\"})\r\n",
        "  return df_standard_scale"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECQB-mPhCmGZ"
      },
      "source": [
        "#Interpolation of missing values:\n",
        "def deal_missing_values(dataframe):\n",
        "  data = dataframe.copy()\n",
        "  for (columnName, columnData) in data.iteritems():\n",
        "    feature = data[columnName]\n",
        "    data[columnName] = feature.interpolate()\n",
        "  return data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OavDm_eil0ID"
      },
      "source": [
        "#Time differencing (t-1):\n",
        "def differencing_transform(dataframe):\n",
        "  data = dataframe.copy()\n",
        "  for (columnName, columnData) in data.iteritems():\n",
        "    data_dif_time = data[columnName]\n",
        "    data[columnName] = data_dif_time - data_dif_time.shift(1)\n",
        "  return data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtKQCr4NhgI-"
      },
      "source": [
        "# Generate multivariate features from n_steps of a group of parallel secuences.\n",
        "def generate_time_features(dataframe, n_steps_in, n_steps_out):\n",
        "  x, y = [], []\n",
        "  for i in range(len(dataframe.index)):\n",
        "      end_step = i + n_steps_in\n",
        "      out_end_ix = end_step + n_steps_out\n",
        "      if out_end_ix > len(dataframe.index)-1:\n",
        "        break\n",
        "      # Separate time feature values from time target values.\n",
        "      x.append(dataframe.iloc[i:end_step, 0:6].values)\n",
        "      # Get only pollutants for target.\n",
        "      y.append(dataframe.iloc[end_step:out_end_ix, 0:6].values)\n",
        "  return np.array(x), np.array(y)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1xpp3ZxUk60"
      },
      "source": [
        "# Split Train/Test dataset:\r\n",
        "def split_train_test(x_time_features, y_time_targets):\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x_time_features, y_time_targets, test_size=0.15, random_state=42, shuffle=True)\r\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QWlhmlXDIR"
      },
      "source": [
        "# Plot variables data.\r\n",
        "def plot_features(dataset):\r\n",
        "  for (columnName, columnData) in dataset.iteritems():\r\n",
        "    plt.figure()\r\n",
        "    print(columnName)\r\n",
        "    dataset[columnName].plot(figsize=(10,10),grid =True)\r\n",
        "    plt.show()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZqTbl3OKn8v"
      },
      "source": [
        "# Search the interval of time that has constant repetead values.\r\n",
        "def find_constantvalue_intervals(dataset, n_constant_max, interlap_max):\r\n",
        "  check_points = []\r\n",
        "  point_break = []\r\n",
        "  # Search for points of time (intervals) that have the same value over \"n_constant_max\" times.\r\n",
        "  for (columnName, columnData) in dataset.iteritems():\r\n",
        "    i = 0\r\n",
        "    while i <= (len(columnData.index)):\r\n",
        "      constant_count = 0\r\n",
        "      for j in range(1,len(columnData.index)):\r\n",
        "        if i+j > len(columnData.index)-1:\r\n",
        "          break\r\n",
        "        # If the next value is the same.\r\n",
        "        if columnData[i] == columnData[i+j]:\r\n",
        "          constant_count = constant_count + 1\r\n",
        "        # If the next value is different.\r\n",
        "        else:\r\n",
        "          break\r\n",
        "      # If number of times value was repeated was more than \"n_constant_max\" times (24 points / 1 day).\r\n",
        "      if constant_count >= n_constant_max:\r\n",
        "        point_break.append((i,i+constant_count))\r\n",
        "        i = i + constant_count\r\n",
        "      else:\r\n",
        "        i = i + 1\r\n",
        "  # Eliminate duplicate intervals between variables.\r\n",
        "  newlist = []\r\n",
        "  for i in point_break:\r\n",
        "      if i not in newlist:\r\n",
        "        newlist.append(i)\r\n",
        "        newlist = sorted(newlist)\r\n",
        "  # Eliminate intervals that are included in others.\r\n",
        "  limits = newlist.copy()\r\n",
        "  for intervals1 in limits:\r\n",
        "    for intervals2 in limits:\r\n",
        "      if intervals2 != intervals1:\r\n",
        "        if ((intervals1[0] <= intervals2[0]) and (intervals1[1] >= intervals2[1])):\r\n",
        "          limits.remove(intervals2)\r\n",
        "      limits = sorted(limits)\r\n",
        "  # Generate union of intervals if there is an intersection between them.\r\n",
        "  constructs = []\r\n",
        "  checkpoints = limits.copy()\r\n",
        "  #Iterate over intersects of intervals.\r\n",
        "  for bis in range(int(len(limits))):\r\n",
        "    for intervals1 in checkpoints:\r\n",
        "      for intervals2 in checkpoints:\r\n",
        "        if intervals2 != intervals1:\r\n",
        "          if (intervals1[1] >= intervals2[0] and intervals1[1] <= intervals2[1]):\r\n",
        "            construct = (intervals1[0],intervals2[1])\r\n",
        "            constructs.append(construct)\r\n",
        "            if intervals1 in checkpoints:\r\n",
        "              checkpoints.remove(intervals1)\r\n",
        "            if intervals2 in checkpoints:\r\n",
        "              checkpoints.remove(intervals2)\r\n",
        "            checkpoints.extend(constructs)\r\n",
        "            constructs = []\r\n",
        "            checkpoints = sorted(checkpoints)\r\n",
        "  # Join intervals with are separeted less than \"interlap_max\" times (n_steps_input + n_steps_output)\r\n",
        "  backup = checkpoints.copy()\r\n",
        "  for bis in range(int(len(backup)/2)):\r\n",
        "    i = 0\r\n",
        "    clean_checkpoints = []\r\n",
        "    while ((i+1) <= len(checkpoints)-1):\r\n",
        "      if ((checkpoints[i+1][0] - checkpoints[i][1]) <= interlap_max):\r\n",
        "        clean_checkpoints.append((checkpoints[i][0],checkpoints[i+1][1]))\r\n",
        "        i = i + 2\r\n",
        "        flag = 1\r\n",
        "      else:\r\n",
        "        clean_checkpoints.append(checkpoints[i])\r\n",
        "        i = i + 1\r\n",
        "        flag = 0\r\n",
        "    if flag == 0:\r\n",
        "      clean_checkpoints.append(checkpoints[len(checkpoints)-1])\r\n",
        "    checkpoints = clean_checkpoints\r\n",
        "\r\n",
        "  print(\"Cantidad de Intervalos:\", len(checkpoints))\r\n",
        "  print(\"Los intervalos de informaciÃ³n constante son:\", checkpoints)\r\n",
        "  return checkpoints"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhnfggN--Efb"
      },
      "source": [
        "# Time difference for time features vectors:\r\n",
        "def time_difference_features(X_features, y_target):\r\n",
        "  Y_sub, X_sub = [], []\r\n",
        "  Y_target_differences, X_features_differences = [], []\r\n",
        "  # Iterate between all the time features.\r\n",
        "  for i in range(len(X_features)):\r\n",
        "    j=0\r\n",
        "    X_sub, Y_sub =  [], []\r\n",
        "    # Calculate first time difference for the target vector (First Value from Target Vector - Last Value from Feature Vector)\r\n",
        "    difference = y_target[i][0] - X_features[i][len(X_features[0][0])-1]\r\n",
        "    # Store difference result in array of arrays.\r\n",
        "    Y_sub.append(difference)\r\n",
        "    while ((j+1) <= (len(X_features[0])-1)):\r\n",
        "      # Calculate time difference from the feature vector (Value from Target Vector(t) - Value from Target Vector(t-1))\r\n",
        "      difference = X_features[i][j+1] - X_features[i][j]\r\n",
        "      # Store difference result in array of arrays.\r\n",
        "      X_sub.append(difference)\r\n",
        "      # Take into account that target vector has less elements than features vector.\r\n",
        "      if ((j+1) <= (len(y_target[0])-1)):\r\n",
        "        # Calculate time difference from the target vector (Value from Feature Vector(t) - Value from Target Vector(t-1))\r\n",
        "        difference = y_target[i][j+1] - y_target[i][j]\r\n",
        "        # Store difference result in array of arrays.\r\n",
        "        Y_sub.append(difference)\r\n",
        "      j=j+1\r\n",
        "    # Store result in array of arrays.\r\n",
        "    X_features_differences.append(X_sub)\r\n",
        "    # Store result in array of arrays.\r\n",
        "    Y_target_differences.append(Y_sub)\r\n",
        "\r\n",
        "  return np.array(X_features_differences), np.array(Y_target_differences)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOE6vXtrbKLw"
      },
      "source": [
        "# Generate multivariate features from n_steps with the first next hour for persistent model from y_test.\r\n",
        "def generate_persistent_features_test(y_test, n_steps_out):\r\n",
        "  y_persistent_test = []\r\n",
        "  for i in range(y_test.shape[0]):\r\n",
        "    next_hour = np.zeros((n_steps_out, 6))\r\n",
        "    next_hour[:,:] = y_test[i][0]\r\n",
        "    y_persistent_test.append(next_hour)\r\n",
        "\r\n",
        "  return np.array(y_persistent_test)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4IcRVYqQ96o"
      },
      "source": [
        "# Generate multivariate features from n_steps with the first next hour for persistent model.\r\n",
        "def generate_persistent_features(dataframe, n_steps_in, n_steps_out):\r\n",
        "  x, y = [], []\r\n",
        "  for i in range(len(dataframe.index)):\r\n",
        "      end_step = i + n_steps_in\r\n",
        "      out_end_ix = end_step + n_steps_out\r\n",
        "      if out_end_ix > len(dataframe.index)-1:\r\n",
        "        break\r\n",
        "      # Separate time feature values from time target values.\r\n",
        "      x.append(dataframe.iloc[i:end_step, :].values)\r\n",
        "      # Get only the next hour pollutants for target.\r\n",
        "      next_hour = np.zeros((n_steps_out, 6))\r\n",
        "      next_hour[:,:] = dataframe.iloc[end_step, 0:6].values\r\n",
        "      y.append(next_hour)\r\n",
        "      \r\n",
        "  return np.array(x), np.array(y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97wU8xApKcAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1e03b04-e15a-42e3-d03c-5fda07e5c188"
      },
      "source": [
        "# Generate train time features dataset from all qHAWAX stations.\n",
        "n_steps_in = 24\n",
        "n_steps_out = 12\n",
        "total_time_features = 0\n",
        "total_time_targets = 0\n",
        "n_stations = 10\n",
        "month_from_test = 10\n",
        "n_constant_max = 24\n",
        "interlap_max = n_steps_out + n_steps_in\n",
        "\n",
        "for i in range(n_stations):\n",
        "  # Change name of file to read.\n",
        "  station_file_number = str(i+1)\n",
        "  station_file_name = \"qHAWAX-VariablesContaminacion_1 (\" + station_file_number + \").csv\"\n",
        "  # Read CSV file.\n",
        "  dataset = pd.read_csv(station_file_name)\n",
        "  # Remove outliers.\n",
        "  data_outliers = remove_outliers(dataset)\n",
        "  # Generate Train time features from time windowing process.\n",
        "  iter, flag = 0, 0\n",
        "  # Search for the intervals in which the data remains constant (same point of data is repeated for more than \"n_constant_max\" times);\n",
        "  # Intervals which have less than \"interlap_max\" values between each of them.\n",
        "  checkpoints = find_constantvalue_intervals(data_outliers, n_constant_max, interlap_max)\n",
        "  data_interval, total_x_station, total_y_station = [], [], []\n",
        "  # If there are intervals of time where the data is constant (no accurate):\n",
        "  if len(checkpoints) > 0:\n",
        "    # First interval of data (from 0 to first checkpoint (A,B) --> Interval = (0,A)).\n",
        "    interval = (0,checkpoints[0][0])\n",
        "    # Do not generate time features if there are not enough registers in the interval.\n",
        "    if (interval[1] - interval[0]) >= interlap_max:\n",
        "      # Build interval of useful data.\n",
        "      data_interval = data_outliers[interval[0]:interval[1]].copy()\n",
        "      # Deal with missing values with interpolation.\n",
        "      data_miss = deal_missing_values(data_interval)\n",
        "      # Generate time features from slice of data.\n",
        "      x_time_features_station, y_time_targets_station = generate_time_features(data_miss, n_steps_in, n_steps_out)\n",
        "      # If there are time features in the first interval.\n",
        "      if (len(x_time_features_station) > 0):\n",
        "        total_x_station = x_time_features_station\n",
        "        total_y_station = y_time_targets_station\n",
        "        flag = 1\n",
        "\n",
        "    while ((iter+1) <= len(checkpoints)-1):\n",
        "      # Interval of data between checkpoints: (A,B);(C,D) --> Interval = (B,C).\n",
        "      interval = (checkpoints[iter][1], checkpoints[iter+1][0])\n",
        "      # Do not generate time features if there are not enough registers in the interval.\n",
        "      if (interval[1] - interval[0]) >= interlap_max:\n",
        "        # Build interval of useful data.\n",
        "        data_interval = data_outliers[interval[0]:interval[1]].copy()\n",
        "        # Deal with missing values with interpolation.\n",
        "        data_miss = deal_missing_values(data_interval)\n",
        "        # Generate time features from slice of data.\n",
        "        x_time_features_station, y_time_targets_station = generate_time_features(data_miss, n_steps_in, n_steps_out)\n",
        "        # If there were NO time features in the first interval but there are in the subsequent intervals.\n",
        "        if ((len(x_time_features_station) > 0) and (flag != 1)):\n",
        "          total_x_station = x_time_features_station\n",
        "          total_y_station = y_time_targets_station \n",
        "          flag = 1\n",
        "        # If there were time features in the first interval and there are in the subsequent intervals.\n",
        "        elif ((len(x_time_features_station) > 0) and (flag == 1)):\n",
        "          total_x_station = np.concatenate((total_x_station, x_time_features_station), axis=0)\n",
        "          total_y_station = np.concatenate((total_y_station, y_time_targets_station), axis=0)\n",
        "      # Iterate through all the time periods with useful data.        \n",
        "      iter = iter + 1\n",
        "\n",
        "    # Last interval of data (from last checkpoint to last dataset register: (C,D) --> Interval = (D,#registros)).  \n",
        "    interval = (checkpoints[len(checkpoints)-1][1], len(data_outliers)-1)\n",
        "    # Do not generate time features if there are not enough registers in the interval.\n",
        "    if (interval[1] - interval[0]) >= interlap_max:\n",
        "      # Build interval of useful data.\n",
        "      data_interval = data_outliers[interval[0]:interval[1]].copy()\n",
        "      # Deal with missing values with interpolation.\n",
        "      data_miss = deal_missing_values(data_interval)\n",
        "      # Generate time features from slice of data.\n",
        "      x_time_features_station, y_time_targets_station = generate_time_features(data_miss, n_steps_in, n_steps_out)\n",
        "      # If there were NO time features in the first interval but there are in the subsequent intervals.\n",
        "      if ((len(x_time_features_station) > 0) and (flag != 1)):\n",
        "        total_x_station = x_time_features_station\n",
        "        total_y_station = y_time_targets_station\n",
        "        flag = 1\n",
        "      # If there were time features in the first interval and there are in the subsequent intervals.\n",
        "      elif ((len(x_time_features_station) > 0) and (flag == 1)):\n",
        "        total_x_station = np.concatenate((total_x_station, x_time_features_station), axis=0)\n",
        "        total_y_station = np.concatenate((total_y_station, y_time_targets_station), axis=0) \n",
        "  # If there are NO intervals of time where the data is constant (no accurate):\n",
        "  else:\n",
        "    total_x_station, total_y_station = generate_time_features(data_outliers, n_steps_in, n_steps_out)\n",
        "  # If it is the first generation of time features.\n",
        "  if (i == 0):\n",
        "    total_time_features = total_x_station\n",
        "    total_time_targets = total_y_station\n",
        "  # If it is not the first generation of time features.\n",
        "  else:\n",
        "    total_time_features = np.concatenate((total_time_features, total_x_station), axis=0)\n",
        "    total_time_targets = np.concatenate((total_time_targets, total_y_station), axis=0)\n",
        "\n",
        "# Print results for Features & Target.\n",
        "print(\"-------------------- Features & Target Dimensions --------------------\")\n",
        "print(\"Size of time features data array:\", total_time_features.shape)\n",
        "print(\"Size of time target data array:\", total_time_targets.shape)\n",
        "\n",
        "# Split Train/Test.\n",
        "X_train, X_test, y_train, y_test = split_train_test(total_time_features, total_time_targets)\n",
        "\n",
        "# Print results after Train/Test.\n",
        "print(\"-------------------- Results after Train/Test Split --------------------\")\n",
        "print(\"Size of time features TRAIN data array:\", X_train.shape)\n",
        "print(\"Size of time features TEST data array:\", X_test.shape)\n",
        "print(\"Size of time target TRAIN data array:\", y_train.shape)\n",
        "print(\"Size of time target TEST data array:\", y_test.shape)\n",
        "\n",
        "# Time Differences.\n",
        "X_train_difference, y_train_difference = time_difference_features(X_train, y_train)\n",
        "X_test_difference, y_test_difference = time_difference_features(X_test, y_test)\n",
        "\n",
        "# Print results after Time Difference Train/Test.\n",
        "print(\"-------------------- Results after Time Difference Train/Test Split --------------------\")\n",
        "print(\"Size of time features TRAIN data array:\", X_train_difference.shape)\n",
        "print(\"Size of time features TEST data array:\", X_test_difference.shape)\n",
        "print(\"Size of time target TRAIN data array:\", y_train_difference.shape)\n",
        "print(\"Size of time target TEST data array:\", y_test_difference.shape)\n",
        "\n",
        "# Stadarizing values.\n",
        "# data_train_scaled, robust_scaled_vector = robust_scaler(X_train, y_train)\n",
        "\n",
        "#plot_features(data_outliers)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cantidad de Intervalos: 4\n",
            "Los intervalos de informaciÃ³n constante son: [(1403, 1447), (1573, 1613), (1747, 1780), (3102, 3129)]\n",
            "Cantidad de Intervalos: 1\n",
            "Los intervalos de informaciÃ³n constante son: [(80, 104)]\n",
            "Cantidad de Intervalos: 4\n",
            "Los intervalos de informaciÃ³n constante son: [(92, 157), (332, 399), (545, 587), (735, 2728)]\n",
            "Cantidad de Intervalos: 10\n",
            "Los intervalos de informaciÃ³n constante son: [(10, 1578), (1793, 1904), (2019, 2255), (2305, 2614), (2659, 2902), (3005, 3041), (3212, 4076), (4212, 4497), (4601, 4689), (4750, 5173)]\n",
            "Cantidad de Intervalos: 17\n",
            "Los intervalos de informaciÃ³n constante son: [(1071, 1112), (1382, 1447), (1529, 1606), (1743, 1782), (1914, 1950), (2075, 2116), (2247, 2283), (2414, 2450), (3097, 3152), (3585, 3617), (3753, 3786), (3919, 3954), (4082, 4116), (4583, 4626), (4743, 4779), (4911, 4947), (5076, 5114)]\n",
            "Cantidad de Intervalos: 0\n",
            "Los intervalos de informaciÃ³n constante son: []\n",
            "Cantidad de Intervalos: 5\n",
            "Los intervalos de informaciÃ³n constante son: [(108, 181), (349, 443), (491, 613), (665, 995), (1050, 2779)]\n",
            "Cantidad de Intervalos: 1\n",
            "Los intervalos de informaciÃ³n constante son: [(0, 2633)]\n",
            "Cantidad de Intervalos: 5\n",
            "Los intervalos de informaciÃ³n constante son: [(0, 631), (681, 1137), (1210, 1810), (1883, 1999), (2077, 3321)]\n",
            "Cantidad de Intervalos: 1\n",
            "Los intervalos de informaciÃ³n constante son: [(3205, 4766)]\n",
            "-------------------- Features & Target Dimensions --------------------\n",
            "Size of time features data array: (27506, 24, 6)\n",
            "Size of time target data array: (27506, 12, 6)\n",
            "-------------------- Results after Train/Test Split --------------------\n",
            "Size of time features TRAIN data array: (23380, 24, 6)\n",
            "Size of time features TEST data array: (4126, 24, 6)\n",
            "Size of time target TRAIN data array: (23380, 12, 6)\n",
            "Size of time target TEST data array: (4126, 12, 6)\n",
            "-------------------- Results after Time Difference Train/Test Split --------------------\n",
            "Size of time features TRAIN data array: (23380, 23, 6)\n",
            "Size of time features TEST data array: (4126, 23, 6)\n",
            "Size of time target TRAIN data array: (23380, 12, 6)\n",
            "Size of time target TEST data array: (4126, 12, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUD_ecX1u3WN"
      },
      "source": [
        "**Modelo de Persistencia y CÃ¡lculo de RMSE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMpx1xiVodkw",
        "outputId": "06d78648-5c2a-436f-9561-8bf75352ae47"
      },
      "source": [
        "y_persistent_test = generate_persistent_features_test(y_test, n_steps_out)\r\n",
        "y_persistent_test.shape\r\n",
        "\r\n",
        "sum_rmse = 0\r\n",
        "rmse_per_variable = []\r\n",
        "for row in range(y_test.shape[0]):  # por cada semana predicha\r\n",
        "  for col in range(y_test.shape[1]): # por cada dia\r\n",
        "    sum_rmse += (y_test[row][col] - y_persistent_test[row][col])**2\r\n",
        "\r\n",
        "for i in range(len(sum_rmse)):\r\n",
        "  rmse_per_variable.append(sqrt(sum_rmse[i] / (y_test.shape[0] * y_test.shape[1])))\r\n",
        "\r\n",
        "rmse_per_variable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[nan, nan, nan, nan, nan, nan]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D35j3asihw8c"
      },
      "source": [
        "**NN Architecture:**\n",
        "- Multi-variate.\n",
        "- Multiple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpg1GNWug0JJ"
      },
      "source": [
        "x_train = X_train_difference\r\n",
        "# Aplana las secuencias de salida\r\n",
        "n_output = y_train_difference.shape[1] * y_train_difference.shape[2] # numero de salidas\r\n",
        "y_train = y_train_difference.reshape((y_train_difference.shape[0], n_output))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeRknQ1waNo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d5f2a9-19e8-4456-fcce-1c05b1c6290d"
      },
      "source": [
        "#TopologÃ­a de CNN\n",
        "n_kernel = 3\n",
        "n_filters = 100\n",
        "# obtiene el numero de series (tercera adimension de X)\n",
        "n_features = x_train.shape[2]\n",
        "n_steps_in_difference = n_steps_in - 1\n",
        "# define arquitectura de modelo MLP\n",
        "modelo = Sequential()\n",
        "modelo.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_steps_in_difference, n_features)))\n",
        "modelo.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n",
        "modelo.add(MaxPooling1D(pool_size=2))\n",
        "modelo.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n",
        "modelo.add(MaxPooling1D(pool_size=2))\n",
        "modelo.add(Flatten())\n",
        "modelo.add(Dense(100, activation='relu'))\n",
        "modelo.add(Dense(n_output))\n",
        "modelo.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "# entrena el modelo MLP con la la data de entrenamiento generada\n",
        "modelo.fit(x_train, y_train, epochs=1000, verbose=0)\n",
        "\n",
        "# muestra un resumen de la topologia del modelo\n",
        "modelo.summary()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 21, 100)           1900      \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 19, 100)           30100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 9, 100)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 7, 100)            30100     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 3, 100)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 72)                7272      \n",
            "=================================================================\n",
            "Total params: 99,472\n",
            "Trainable params: 99,472\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnE6-2jF7ITo",
        "outputId": "f635006a-994a-4dfa-e0fc-16af582a561c"
      },
      "source": [
        "n_features = X_test_difference.shape[2]\r\n",
        "n_steps_in_difference = X_test_difference.shape[1]\r\n",
        "\r\n",
        "X_input = X_test_difference[0]\r\n",
        "\r\n",
        "X_input = X_input.reshape(1, n_steps_in_difference, n_features)\r\n",
        "\r\n",
        "yhat = modelo.predict(X_input, verbose=0)\r\n",
        "yhat"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
              "        nan, nan, nan, nan, nan, nan, nan]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDLwK8XGwlql",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "305dcd11-dade-4e28-b044-ec6d201fff99"
      },
      "source": [
        "#Pendientes\r\n",
        "- Modelo Base de Persistencia. - (OK)\r\n",
        "- FunciÃ³n para evaluar desempeÃ±o de NN (RMSE) - (OK)\r\n",
        "- NormalizaciÃ³n Robusta de ParÃ¡metros\r\n",
        "- Â¿QuÃ© hacer con los valores NaN?\r\n",
        "- CNN --> Conv2D?\r\n",
        "\r\n",
        "- Eliminar en base a outliers de boxplot (ecuaciÃ³n de cuartiles)\r\n",
        "- Imputar los valores <6 con interpolaciÃ³n lineal\r\n",
        "- Valores nulos > 20 no considerarlos (calcular time features por bloque)\r\n",
        "- Time differences en persisntencia debe ser 0\r\n",
        "- trabajar todo en original y el paso de diferenciaciÃ³n en data "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-138-1222b4442659>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    - Modelo Base de Persistencia. - (OK)\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}